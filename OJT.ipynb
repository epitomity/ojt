{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6428875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "from gensim.models import CoherenceModel\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.utils import simple_preprocess\n",
    "from pprint import pprint\n",
    "\n",
    "from bitermplus import BTM\n",
    "\n",
    "import bitermplus as btm\n",
    "import tmplot as tmp\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LDA==========================================================================================================\n",
    "#https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "#read the txt file\n",
    "with open(r'cleaned_tweets.txt') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# create a dataframe\n",
    "df = pd.DataFrame(data, columns=['text'])\n",
    "\n",
    "# write dataframe to csv file\n",
    "df.to_csv('cleaned_tweets.csv', index=False)\n",
    "data = pd.read_csv('cleaned_tweets.csv')\n",
    "# preprocess text data\n",
    "stop_words = stopwords.words('English')\n",
    "TL_stopwords = open(\"Stopwords_EN_TL.txt\").read().splitlines() \n",
    "stop_words.extend(TL_stopwords)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in stop_words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "data['processed_text'] = data['text'].apply(preprocess)\n",
    "\n",
    "# # Join the different processed titles together.\n",
    "# long_string = ','.join(data['text'] )\n",
    "# # Create a WordCloud object\n",
    "# wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "# # Generate a word cloud\n",
    "# wordcloud.generate(long_string)\n",
    "# # Visualize the word cloud\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "# Tokenize the documents\n",
    "tokenized_docs = [doc.split() for doc in data]\n",
    "# create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(data['processed_text'])\n",
    "corpus = [dictionary.doc2bow(text) for text in data['processed_text']]\n",
    "#  # Define the range of hyperparameter values to search over\n",
    "# num_topics_list = [3,5,10]\n",
    "# passes_list = [3,5,10]\n",
    "# alpha_list = ['symmetric',0.3,0.5]\n",
    "\n",
    "# # Perform a grid search over the hyperparameter values\n",
    "# results = []\n",
    "# for num_topics, passes , alpha in product(num_topics_list, passes_list,alpha_list):\n",
    "#     lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes,alpha=alpha,)\n",
    "#     coherence_model = CoherenceModel(model=lda_model, texts=data['processed_text'], dictionary=dictionary, coherence='c_v')\n",
    "#     coherence_score = coherence_model.get_coherence()\n",
    "#     results.append({'num_topics': num_topics,'alpha':alpha, 'passes': passes, 'coherence_score': coherence_score})\n",
    "\n",
    "# #Find the hyperparameters with the highest coherence score\n",
    "# best_params = max(results, key=lambda x: x['coherence_score'])\n",
    "# print('Best parameters:', best_params)\n",
    "\n",
    "\n",
    "#Best parameters: {'num_topics': 3, 'alpha': 0.3, 'passes': 10, 'coherence_score': 0.6115592649171243} runtime:4omins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ccbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, alpha=0.3,passes=10)\n",
    "\n",
    "# Compute the coherence score for the trained LDA model\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=data['processed_text'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print('Coherence score:', coherence_score)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aacaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Biterm========================================================================================================\n",
    "#https://bitermplus.readthedocs.io/en/latest/index.html\n",
    "#IMPORTING DATA\n",
    "df = pd.read_csv(\n",
    "    'cleaned_tweets.csv', header=None, names=['texts'])\n",
    "# preprocess text data\n",
    "stop_words = stopwords.words('English')\n",
    "TL_stopwords = open(\"Stopwords_EN_TL.txt\").read().splitlines() \n",
    "stop_words.extend(TL_stopwords)\n",
    "texts = df['texts'].str.strip().tolist()\n",
    "# Obtaining terms frequency in a sparse matrix and corpus vocabulary\n",
    "X, vocabulary, vocab_dict = btm.get_words_freqs(texts,stop_words=stop_words)\n",
    "tf = np.array(X.sum(axis=0)).ravel()\n",
    "# Vectorizing documents\n",
    "docs_vec = btm.get_vectorized_docs(texts, vocabulary)\n",
    "docs_lens = list(map(len, docs_vec))\n",
    "# Generating biterms\n",
    "biterms = btm.get_biterms(docs_vec)\n",
    "#after using grid search the best param was:\n",
    "#M=10, alpha=0.01, beta=0.01 = coherence(-6435),perplexity(5020)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZING AND RUNNING MODEL\n",
    "model = btm.BTM(\n",
    "    X, vocabulary, T=10, M=10, alpha=0.01, beta=0.01)\n",
    "model.fit(biterms, iterations=20)\n",
    "p_zd = model.transform(docs_vec)\n",
    "\n",
    "#METRICS\n",
    "perplexity = btm.perplexity(model.matrix_topics_words_, p_zd, X, 8)\n",
    "coherence = btm.coherence(model.matrix_topics_words_, X, M=20)\n",
    "\n",
    "topics = btm.get_top_topic_words(\n",
    "    model,\n",
    "    words_num=10,)\n",
    "print(topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c641b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m# print top 10 words for each cluster\u001b[39;00m\n\u001b[0;32m     33\u001b[0m TL_stopwords \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStopwords_EN_TL.txt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39msplitlines() \n\u001b[1;32m---> 34\u001b[0m stop_words\u001b[39m.\u001b[39mextend(TL_stopwords)\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m cluster \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "#distilbert===================================================================================================\n",
    "\n",
    "# load data into a pandas dataframe\n",
    "df = pd.read_csv('cleaned_tweets.csv')\n",
    "# clean data\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# tokenize text data using DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "df['text'] = df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "# extract embeddings using DistilBERT model\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "embeddings = []\n",
    "for text in df['text']:\n",
    "    input_ids = torch.tensor([text])\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "        embeddings.append(output[0][0][0].numpy())\n",
    "df['embeddings'] = embeddings\n",
    "# reduce dimensionality of embeddings using UMAP\n",
    "reducer = umap.UMAP(n_neighbors=15, n_components=5, metric='cosine')\n",
    "umap_embeddings = reducer.fit_transform(df['embeddings'].tolist())\n",
    "df['umap_embeddings'] = umap_embeddings.tolist()\n",
    "# preprocess data\n",
    "df['text'] = df['text'].apply(lambda x: str(x).lower())\n",
    "\n",
    "# perform clustering using HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, metric='euclidean')\n",
    "clusters = clusterer.fit_predict(df['umap_embeddings'].tolist())\n",
    "df['cluster'] = clusters\n",
    "\n",
    "# print top 10 words for each cluster\n",
    "stop_words = stopwords.words('English')\n",
    "TL_stopwords = open(\"Stopwords_EN_TL.txt\").read().splitlines() \n",
    "stop_words.extend(TL_stopwords)\n",
    "if 'cluster' in df.columns:\n",
    "    for cluster in sorted(df['cluster'].unique()):\n",
    "        texts = df[df['cluster'] == cluster]['text']\n",
    "        if len(texts) > 1:\n",
    "            vectorizer = CountVectorizer(stop_words=TL_stopwords, max_features=5000)\n",
    "            X = vectorizer.fit_transform(texts)\n",
    "            lda = LatentDirichletAllocation(n_components=1, random_state=42)\n",
    "            lda.fit(X)\n",
    "            feature_names = vectorizer.get_feature_names()\n",
    "            top_words = [feature_names[i] for i in lda.components_[0].argsort()[:-11:-1]]\n",
    "            print(f'Topic {cluster}: {\" | \".join(top_words)}')\n",
    "else:\n",
    "    print('Error: \"cluster\" column not found in DataFrame.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66734f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n==========================================COHERENCE=============================================\\n\\n\")\n",
    "\n",
    "#Reg exp for tokenizing the data set\n",
    "tokenizer = lambda s: re.findall('\\w+', s.lower())\n",
    "text = [tokenizer(t) for t in data]\n",
    "\n",
    "# Getting Topics\n",
    "all_topics = topic_model.get_topics()\n",
    "top = []\n",
    "keys = []\n",
    "for x in range(10):\n",
    "    keys.append(freq['Topic'].head(10)[x])\n",
    "\n",
    "#Tokenizing\n",
    "prefix = 'Getting Topics'\n",
    "pbar2 = tqdm(total=len(keys), position=0, leave=True)\n",
    "pbar2.set_description(prefix)\n",
    "for key in tqdm(keys, desc='Getting Topics', position=0, leave=True):\n",
    "    values = all_topics[key]\n",
    "    topic_1 = []\n",
    "    for value in tqdm(values, desc='Retrieving Values in topic ' + str(key), position=0, leave=True):\n",
    "        topic_1.append(value[0])\n",
    "    top.append(topic_1)\n",
    "\n",
    "# Creating a dictionary with the vocabulary\n",
    "word2id = Dictionary(text)\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(data).toarray()\n",
    "vocab = np.array(vec.get_feature_names())\n",
    "# Coherence model\n",
    "cm = CoherenceModel(topics=top, texts=text, coherence='u_mass', dictionary=word2id)\n",
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "#Results\n",
    "print(\"\\n==========================================COHERENCE RESULTS=============================================\\n\")\n",
    "for index, x in enumerate(coherence_per_topic):\n",
    "    print(\"topic %2d : %5.2f\" % (index + 1, x))\n",
    "\n",
    "coherence = cm.get_coherence()\n",
    "print(coherence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bertopic===================================================================================================\n",
    "\n",
    "# Read Stopwords_EN_TL.txt and save it into a pandas DataFrame\n",
    "stop_words_dataframe = pd.read_csv(\"Stopwords_EN_TL.txt\")\n",
    "stop_words = set(stop_words_dataframe.iloc[:,0])\n",
    "# Read csv and save into a pandas DataFrame\n",
    "docs_dataframe = pd.read_csv(\"cleaned_tweets.txt\")\n",
    "# Remove stopwords for every comment and clean the dataset\n",
    "docs = []\n",
    "index = 0\n",
    "for w in docs_dataframe.iloc[:,0].items():\n",
    "    series = hero.remove_stopwords(pd.Series(w[1]),stop_words)\n",
    "    series = hero.preprocessing.clean(series)\n",
    "    docs.append(series[0])\n",
    "# Output the cleaned dataset to an excel file\n",
    "cleaned_dataset = pd.DataFrame(docs)\n",
    "cleaned_dataset.to_excel(\"cleaned_tweets.xlsx\")\n",
    "# Initialize the model and fit it to the data\n",
    "\n",
    "# Hyperparameters:\n",
    "# language - \"english\" or \"multilingual\"\n",
    "# top_n_words - the top_n_words in each topic (no effect)\n",
    "# n_gram_range - the n-gram to be used by the vectorizer in the model (no effect / incoherent)\n",
    "# min_topic_size - how big a topic should be, adjusted to be similar to LDA\n",
    "# nr_topics - topic reduction, made topics more incoherent\n",
    "\n",
    "topic_model = BERTopic(min_topic_size=25, language = \"multilingual\")\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "# Print the topics found by the model\n",
    "topics = topic_model.get_topic_info()\n",
    "topics.to_excel(\"output.xlsx\")\n",
    "topics\n",
    "# Extract vectorizer and tokenizer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "tokens = [tokenizer(doc) for doc in docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words,\n",
    "                                 texts=tokens,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "\n",
    "# Print Coherence\n",
    "coherence = coherence_model.get_coherence()\n",
    "coherence\n",
    "topic_model.visualize_barchart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc469cb1447c57164a91d86ef58a8cb648499c4b69f097fb6c8d58ebda93f3a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
