{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86516baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6428875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import bitermplus as btm\n",
    "import tmplot as tmp\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#LDA==========================================================================================================\n",
    "#https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "#read the txt file\n",
    "with open(r'cleaned_tweets.txt') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# create a dataframe\n",
    "df = pd.DataFrame(data, columns=['text'])\n",
    "\n",
    "# write dataframe to csv file\n",
    "df.to_csv('cleaned_tweets.csv', index=False)\n",
    "data = pd.read_csv('cleaned_tweets.csv')\n",
    "\n",
    "# preprocess text data\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in stop_words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "data['processed_text'] = data['text'].apply(preprocess)\n",
    "\n",
    "# create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(data['processed_text'])\n",
    "corpus = [dictionary.doc2bow(text) for text in data['processed_text']]\n",
    "\n",
    "# train LDA model\n",
    "num_topics = 10\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)\n",
    "\n",
    "# print top topics and their keywords\n",
    "for topic in lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
    "    print('Topic {}: {}'.format(topic[0], [word[0] for word in topic[1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aacaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Biterm========================================================================================================\n",
    "#https://bitermplus.readthedocs.io/en/latest/index.html\n",
    "#IMPORTING DATA\n",
    "df = pd.read_csv(\n",
    "    'cleaned_tweets.csv', header=None, names=['texts'])\n",
    "texts = df['texts'].str.strip().tolist()\n",
    "# PREPROCESSING\n",
    "# Obtaining terms frequency in a sparse matrix and corpus vocabulary\n",
    "X, vocabulary, vocab_dict = btm.get_words_freqs(texts)\n",
    "tf = np.array(X.sum(axis=0)).ravel()\n",
    "# Vectorizing documents\n",
    "docs_vec = btm.get_vectorized_docs(texts, vocabulary)\n",
    "docs_lens = list(map(len, docs_vec))\n",
    "# Generating biterms\n",
    "biterms = btm.get_biterms(docs_vec)\n",
    "\n",
    "# INITIALIZING AND RUNNING MODEL\n",
    "model = btm.BTM(\n",
    "    X, vocabulary, seed=12321, T=10, M=20, alpha=50/8, beta=0.01)\n",
    "model.fit(biterms, iterations=20)\n",
    "p_zd = model.transform(docs_vec)\n",
    "\n",
    "#METRICS\n",
    "perplexity = btm.perplexity(model.matrix_topics_words_, p_zd, X, 8)\n",
    "coherence = btm.coherence(model.matrix_topics_words_, X, M=20)\n",
    "\n",
    "topics = btm.get_top_topic_words(\n",
    "    model,\n",
    "    words_num=10,)\n",
    "print(topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c641b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 32 but got size 12 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_tweets.txt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     25\u001b[0m     data \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m---> 27\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m lda, dtm, vectorizer \u001b[38;5;241m=\u001b[39m perform_lda(embeddings, num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, topic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lda\u001b[38;5;241m.\u001b[39mcomponents_):\n",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embeddings\u001b[39m(documents):\n\u001b[0;32m      8\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m [tokenizer(doc, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m----> 9\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokenized])\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 32 but got size 12 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "#distilbert===================================================================================================\n",
    "# load the DistilBERT model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#get document\n",
    "def get_embeddings(documents):\n",
    "    tokenized = [tokenizer(doc, padding=True, truncation=True, max_length=1512, return_tensors='pt') for doc in documents]\n",
    "    input_ids = torch.cat([t['input_ids'] for t in tokenized])\n",
    "    attention_mask = torch.cat([t['attention_mask'] for t in tokenized])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.numpy()\n",
    "#perform LDA in document\n",
    "def perform_lda(embeddings, num_topics):\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(embeddings)\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "    return lda, dtm, vectorizer\n",
    "#train\n",
    "#read the txt file\n",
    "df = pd.read_csv(\n",
    "    'cleaned_tweets.csv', header=None, names=['texts'])\n",
    "texts = df['texts'].str.strip().tolist()\n",
    "\n",
    "embeddings = get_embeddings(texts)\n",
    "lda, dtm, vectorizer = perform_lda(embeddings, num_topics=2)\n",
    "\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[:-5-1:-1]\n",
    "    top_words = [vectorizer.get_feature_names()[i] for i in top_words_idx]\n",
    "    print(f'Topic {i}: {\", \".join(top_words)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66734f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n==========================================COHERENCE=============================================\\n\\n\")\n",
    "\n",
    "#Reg exp for tokenizing the data set\n",
    "tokenizer = lambda s: re.findall('\\w+', s.lower())\n",
    "text = [tokenizer(t) for t in data]\n",
    "\n",
    "# Getting Topics\n",
    "all_topics = topic_model.get_topics()\n",
    "top = []\n",
    "keys = []\n",
    "for x in range(10):\n",
    "    keys.append(freq['Topic'].head(10)[x])\n",
    "\n",
    "#Tokenizing\n",
    "prefix = 'Getting Topics'\n",
    "pbar2 = tqdm(total=len(keys), position=0, leave=True)\n",
    "pbar2.set_description(prefix)\n",
    "for key in tqdm(keys, desc='Getting Topics', position=0, leave=True):\n",
    "    values = all_topics[key]\n",
    "    topic_1 = []\n",
    "    for value in tqdm(values, desc='Retrieving Values in topic ' + str(key), position=0, leave=True):\n",
    "        topic_1.append(value[0])\n",
    "    top.append(topic_1)\n",
    "\n",
    "# Creating a dictionary with the vocabulary\n",
    "word2id = Dictionary(text)\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(data).toarray()\n",
    "vocab = np.array(vec.get_feature_names())\n",
    "# Coherence model\n",
    "cm = CoherenceModel(topics=top, texts=text, coherence='u_mass', dictionary=word2id)\n",
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "#Results\n",
    "print(\"\\n==========================================COHERENCE RESULTS=============================================\\n\")\n",
    "for index, x in enumerate(coherence_per_topic):\n",
    "    print(\"topic %2d : %5.2f\" % (index + 1, x))\n",
    "\n",
    "coherence = cm.get_coherence()\n",
    "print(coherence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bertopic===================================================================================================\n",
    "\n",
    "# Read Stopwords_EN_TL.txt and save it into a pandas DataFrame\n",
    "stop_words_dataframe = pd.read_csv(\"Stopwords_EN_TL.txt\")\n",
    "stop_words = set(stop_words_dataframe.iloc[:,0])\n",
    "# Read csv and save into a pandas DataFrame\n",
    "docs_dataframe = pd.read_csv(\"cleaned_tweets.txt\")\n",
    "# Remove stopwords for every comment and clean the dataset\n",
    "docs = []\n",
    "index = 0\n",
    "for w in docs_dataframe.iloc[:,0].items():\n",
    "    series = hero.remove_stopwords(pd.Series(w[1]),stop_words)\n",
    "    series = hero.preprocessing.clean(series)\n",
    "    docs.append(series[0])\n",
    "# Output the cleaned dataset to an excel file\n",
    "cleaned_dataset = pd.DataFrame(docs)\n",
    "cleaned_dataset.to_excel(\"cleaned_tweets.xlsx\")\n",
    "# Initialize the model and fit it to the data\n",
    "\n",
    "# Hyperparameters:\n",
    "# language - \"english\" or \"multilingual\"\n",
    "# top_n_words - the top_n_words in each topic (no effect)\n",
    "# n_gram_range - the n-gram to be used by the vectorizer in the model (no effect / incoherent)\n",
    "# min_topic_size - how big a topic should be, adjusted to be similar to LDA\n",
    "# nr_topics - topic reduction, made topics more incoherent\n",
    "\n",
    "topic_model = BERTopic(min_topic_size=25, language = \"multilingual\")\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "# Print the topics found by the model\n",
    "topics = topic_model.get_topic_info()\n",
    "topics.to_excel(\"output.xlsx\")\n",
    "topics\n",
    "# Extract vectorizer and tokenizer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "tokens = [tokenizer(doc) for doc in docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words,\n",
    "                                 texts=tokens,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "\n",
    "# Print Coherence\n",
    "coherence = coherence_model.get_coherence()\n",
    "coherence\n",
    "topic_model.visualize_barchart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
